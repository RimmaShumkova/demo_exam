{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модели для классификации\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RidgeClassifier': RidgeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0, allow_writing_files=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(trial, model_name):\n",
    "    if model_name == 'RidgeClassifier':\n",
    "        return {\n",
    "            'alpha': trial.suggest_float('alpha', 0.01, 100.0, log=True),\n",
    "            'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'])\n",
    "        }\n",
    "    elif model_name == 'RandomForest':\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "        }\n",
    "    elif model_name == 'GradientBoosting':\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "        }\n",
    "    elif model_name == 'CatBoost':\n",
    "        return {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1)\n",
    "        }\n",
    "    else:  \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted'),\n",
    "    'Recall': lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted'),\n",
    "    'F1': lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "best_scores = {}\n",
    "pca_versions = {\n",
    "    'std': X_train_pca_std,\n",
    "    'minmax': X_train_pca_minmax, \n",
    "    'robust': X_train_pca_robust\n",
    "}\n",
    "\n",
    "for model_name in models.keys():\n",
    "    model_best_score = 0\n",
    "    model_best_version = None\n",
    "    model_best_params = None\n",
    "    model_best_instance = None\n",
    "    \n",
    "    print(f\"\\nПодбор гиперпараметров для {model_name}\")\n",
    "    \n",
    "    for version_name, X_train_data in pca_versions.items():\n",
    "        print(f\"Версия данных: {version_name}\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = get_model_params(trial, model_name)\n",
    "            model = clone(models[model_name])\n",
    "            model.set_params(**params)\n",
    "            \n",
    "            kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(model, X_train_data, y_train, cv=kf, scoring='accuracy')\n",
    "            return scores.mean()\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=3, show_progress_bar=True)\n",
    "        \n",
    "        current_accuracy = study.best_value\n",
    "        \n",
    "        print(f\"Лучшие параметры: {study.best_params}\")\n",
    "        print(f\"Лучшая Accuracy: {current_accuracy:.4f}\")\n",
    "        \n",
    "        if current_accuracy > model_best_score:\n",
    "            model_best_score = current_accuracy\n",
    "            model_best_version = version_name\n",
    "            model_best_params = study.best_params\n",
    "            \n",
    "            final_model = clone(models[model_name])\n",
    "            final_model.set_params(**study.best_params)\n",
    "            final_model.fit(X_train_data, y_train)\n",
    "            model_best_instance = final_model\n",
    "    \n",
    "    print(f\"Лучшая версия для {model_name}: {model_best_version} с Accuracy: {model_best_score:.4f}\")\n",
    "    \n",
    "    best_models[model_name] = {\n",
    "        'model': model_best_instance,\n",
    "        'version': model_best_version,\n",
    "        'params': model_best_params\n",
    "    }\n",
    "    best_scores[model_name] = model_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_mapping = {\n",
    "    'std': (X_train_pca_std, X_test_pca_std),\n",
    "    'minmax': (X_train_pca_minmax, X_test_pca_minmax),\n",
    "    'robust': (X_train_pca_robust, X_test_pca_robust)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_info in best_models.items():\n",
    "    model = model_info['model']\n",
    "    best_version = model_info['version']\n",
    "    \n",
    "    X_train_data, X_test_data = pca_data_mapping[best_version]\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_data)\n",
    "    y_test_pred = model.predict(X_test_data)\n",
    "    \n",
    "    train_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Dataset': 'Train',\n",
    "        'PCA_Version': best_version,\n",
    "        'Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'Precision': precision_score(y_train, y_train_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_train, y_train_pred, average='weighted'),\n",
    "        'F1': f1_score(y_train, y_train_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    test_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Dataset': 'Test',\n",
    "        'PCA_Version': best_version,\n",
    "        'Accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'Precision': precision_score(y_test, y_test_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_test_pred, average='weighted'),\n",
    "        'F1': f1_score(y_test, y_test_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    results.append(train_metrics)\n",
    "    results.append(test_metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nТаблица метрик лучших версий PCA по моделям:\")\n",
    "print(f\"{'Модель':<20} {'Версия PCA':<12} {'Выборка':<8} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "\n",
    "results_sorted = results_df.sort_values(['Model', 'Dataset'])\n",
    "\n",
    "for idx, row in results_sorted.iterrows():\n",
    "    print(f\"{row['Model']:<20} {row['PCA_Version']:<12} {row['Dataset']:<8} \"\n",
    "          f\"{row['Accuracy']:9.4f}  {row['Precision']:9.4f}  {row['Recall']:9.4f}  {row['F1']:9.4f}\")\n",
    "\n",
    "test_results = results_df[results_df['Dataset'] == 'Test']\n",
    "overall_best = test_results.loc[test_results['Accuracy'].idxmax()]\n",
    "\n",
    "print(\"\\nЛучшая модель на тестовой выборке:\")\n",
    "print(f\"Модель: {overall_best['Model']}\")\n",
    "print(f\"Версия PCA: {overall_best['PCA_Version']}\")\n",
    "print(f\"Accuracy: {overall_best['Accuracy']:.4f}\")\n",
    "print(f\"Precision: {overall_best['Precision']:.4f}\")\n",
    "print(f\"Recall: {overall_best['Recall']:.4f}\")\n",
    "print(f\"F1: {overall_best['F1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
